{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental pipeline in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before our experimental pipeline kicks off, some global parameters need to be defined. We devised a manual\n",
    "5-fold out-of-time validation, by dividing the dataset based on a rolling window approach. `timeframe` specifes which timeframe is selected. The `undersampling_rate` indicates the graph-level undersampling rate defined as the desired ratio of fraudulent transactions over legitimate ones. The `embedding_size` defines the dimension of the embeddings learned by our inductive graph representation learners and `add_additional_data` is a boolean indicating whether or not we would like to add the original transaction features to the transaction node embeddings before training and evaluating our downstream classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters:\n",
    "timeframe = 4\n",
    "undersampling_rate = None\n",
    "embedding_size = 64\n",
    "add_additional_data = True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import timedelta\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "\n",
    "class Timeframes:\n",
    "\n",
    "    \"\"\"\n",
    "    This class initializes the a rolling window timeframe configuration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_column : Pandas dataframe\n",
    "        A one column pandas dataframe containing the dates associated with records.\n",
    "    step_size : datetime.timedelta\n",
    "        Step size (days) defines how many days are between the start days of two consecutive timeframes.\n",
    "    window_size: datetime.timedelta\n",
    "        Defnes the window size (days) of a timeframe.\n",
    "    \"\"\"     \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, date_column, step_size=None, train_size=None, test_size=None):\n",
    "        \n",
    "        if not isinstance(step_size, timedelta):\n",
    "            raise ValueError('step_size should be of type timedelta')\n",
    "\n",
    "        if not isinstance(train_size, timedelta):\n",
    "            raise ValueError('train_size should be of type timedelta')\n",
    "\n",
    "        if not isinstance(test_size, timedelta):\n",
    "            raise ValueError('test_size should be of type timedelta')\n",
    "\n",
    "        self.date_column = date_column\n",
    "        self.step_size = step_size\n",
    "        self.train_size = train_size\n",
    "        self.test_size = test_size\n",
    "        self.fold_size = self.train_size + self.test_size\n",
    "    \n",
    "    def get_number_of_days(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function returns the number of days covered by your data.\n",
    "        The number of days can be used to identify a desired step and window size. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        date_column_name = list(self.date_column.columns)[0]\n",
    "        return (max(self.date_column[date_column_name])-min(self.date_column[date_column_name])).days\n",
    "    \n",
    "    def get_number_of_timeframes(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function returns the number of timeframes that can be created based on the configured step and window size. \n",
    "        \n",
    "        \"\"\"\n",
    "        return int((1+ ((self.get_number_of_days()+1)-self.window_size)/self.step_size)//1)\n",
    "\n",
    "    def train_inductive_split(self, data, hold_out_days):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function returns two pandas dataframes: train data and inductive data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Pandas dataframe\n",
    "            The data that needs to be split\n",
    "        hold_out_days : int\n",
    "            The number of days that should be held out of the train set.\n",
    "\n",
    "        \"\"\"       \n",
    "        if hold_out_days > self.window_size:\n",
    "            raise ValueError(\"the number of hold out days cannot be larger than the total number of days in the window.\")\n",
    "            return\n",
    "        date_column_name = list(self.date_column.columns)[0]\n",
    "        timeframe_data = self.date_column.loc[data.index]\n",
    "        end_date = dateparser.parse(max(timeframe_data[date_column_name]).strftime('%Y-%m-%d'))+timedelta(1)\n",
    "        train_indices = (timeframe_data[timeframe_data[date_column_name] < (end_date-timedelta(hold_out_days))]).index\n",
    "        inductive_indices = (timeframe_data[timeframe_data[date_column_name] >= (end_date-timedelta(hold_out_days))]).index\n",
    "\n",
    "        return data.loc[train_indices], data.loc[inductive_indices]\n",
    "    \n",
    "    def split(self): \n",
    "\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        first_timestamp = min(self.date_column)\n",
    "        last_timestamp = max(self.date_column)\n",
    "        print(last_timestamp)\n",
    "        print(type(last_timestamp))\n",
    "        current_timestamp = first_timestamp\n",
    "        while current_timestamp < last_timestamp - self.fold_size:\n",
    "            print('next')\n",
    "            start_train = current_timestamp\n",
    "            end_train = current_timestamp + self.train_size\n",
    "            start_test = end_train\n",
    "            end_test = start_test + self.test_size\n",
    "\n",
    "            train_indices.append(((self.date_column >= start_train) & (self.date_column < end_train)).nonzero()[0])\n",
    "            test_indices.append(((self.date_column >= start_test) & (self.date_column < end_test)).nonzero()[0])\n",
    "\n",
    "            current_timestamp = current_timestamp + self.step_size\n",
    "\n",
    "        return train_indices, test_indices\n",
    "    \n",
    "    def get_timeframe_indices(self, timeframe):\n",
    "        \"\"\"\n",
    "        This function returns the indices associated with a certain timeframe.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        timeframe : int\n",
    "            The numeric identifier of the timeframe for which the indices are requested. \n",
    "\n",
    "        \"\"\"       \n",
    "        date_column_name = list(self.date_column.columns)[0]\n",
    "        self.date_column[date_column_name] = pd.to_datetime(self.date_column[date_column_name])\n",
    "        start_date = dateparser.parse(min(self.date_column[date_column_name]).strftime('%Y-%m-%d'))\n",
    "        st = (timeframe-1)*self.step_size\n",
    "        timeframe_records = self.date_column[self.date_column[date_column_name] >= start_date +timedelta(st)]\n",
    "        timeframe_records = timeframe_records[timeframe_records[date_column_name] < (start_date + (timedelta(self.window_size+st)))]\n",
    "\n",
    "        \n",
    "        return timeframe_records.index  \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the Credit Card Transaction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load numeric, preprocessed transaction data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/raf/googledrive/DOC/data/FUCC/ccf_preprocessed.csv\", index_col = \"Unnamed: 0\", parse_dates=[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_ID</th>\n",
       "      <th>CARD_PAN_ID</th>\n",
       "      <th>TERM_MIDUID</th>\n",
       "      <th>TERM_MCC</th>\n",
       "      <th>TERM_COUNTRY</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>TX_ACCEPTED</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>COUNTRY_CATEGORY</th>\n",
       "      <th>...</th>\n",
       "      <th>COUNTRY_EU</th>\n",
       "      <th>COUNTRY_NB</th>\n",
       "      <th>COUNTRY_ROW</th>\n",
       "      <th>COUNTRY_USA</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1</td>\n",
       "      <td>cF795A68EEDA3B68BFBB543B082EC9607C3B13C2A0190E...</td>\n",
       "      <td>m842904041e+06</td>\n",
       "      <td>7399</td>\n",
       "      <td>BEL</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2013-10-01 01:00:01</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BE</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2</td>\n",
       "      <td>cAC83FDA148FE02DF7A582131ABE4AF1BD7589A13C7521...</td>\n",
       "      <td>m0001740300759911</td>\n",
       "      <td>4816</td>\n",
       "      <td>USA</td>\n",
       "      <td>7.37</td>\n",
       "      <td>2013-10-01 01:00:06</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3</td>\n",
       "      <td>c1CD10E1CC65E651ECFA881CA89A62DE06CAF75077D643...</td>\n",
       "      <td>m2070050002009250</td>\n",
       "      <td>5735</td>\n",
       "      <td>LUX</td>\n",
       "      <td>6.25</td>\n",
       "      <td>2013-10-01 01:00:08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t4</td>\n",
       "      <td>c4ECA551366385232A79A5249733E42BB685F0E6237F0A...</td>\n",
       "      <td>m003020378135866130847</td>\n",
       "      <td>7523</td>\n",
       "      <td>CAN</td>\n",
       "      <td>7.18</td>\n",
       "      <td>2013-10-01 01:00:08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ROW</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5</td>\n",
       "      <td>c74186F480DC2BED64AEC74D265B2B46C2EBA1BFE4F5AC...</td>\n",
       "      <td>m8000009421670050</td>\n",
       "      <td>4812</td>\n",
       "      <td>USA</td>\n",
       "      <td>154.93</td>\n",
       "      <td>2013-10-01 01:00:09</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  TX_ID                                        CARD_PAN_ID  \\\n",
       "0    t1  cF795A68EEDA3B68BFBB543B082EC9607C3B13C2A0190E...   \n",
       "1    t2  cAC83FDA148FE02DF7A582131ABE4AF1BD7589A13C7521...   \n",
       "2    t3  c1CD10E1CC65E651ECFA881CA89A62DE06CAF75077D643...   \n",
       "3    t4  c4ECA551366385232A79A5249733E42BB685F0E6237F0A...   \n",
       "4    t5  c74186F480DC2BED64AEC74D265B2B46C2EBA1BFE4F5AC...   \n",
       "\n",
       "              TERM_MIDUID  TERM_MCC TERM_COUNTRY  TX_AMOUNT  \\\n",
       "0          m842904041e+06      7399          BEL       0.00   \n",
       "1       m0001740300759911      4816          USA       7.37   \n",
       "2       m2070050002009250      5735          LUX       6.25   \n",
       "3  m003020378135866130847      7523          CAN       7.18   \n",
       "4       m8000009421670050      4812          USA     154.93   \n",
       "\n",
       "          TX_DATETIME  TX_ACCEPTED  TX_FRAUD COUNTRY_CATEGORY  ...  \\\n",
       "0 2013-10-01 01:00:01        False     False               BE  ...   \n",
       "1 2013-10-01 01:00:06         True     False              USA  ...   \n",
       "2 2013-10-01 01:00:08         True     False               NB  ...   \n",
       "3 2013-10-01 01:00:08         True     False              ROW  ...   \n",
       "4 2013-10-01 01:00:09         True     False              USA  ...   \n",
       "\n",
       "   COUNTRY_EU  COUNTRY_NB  COUNTRY_ROW  COUNTRY_USA  month  day  hour  minute  \\\n",
       "0           0           0            0            0     10    1     1       0   \n",
       "1           0           0            0            1     10    1     1       0   \n",
       "2           0           1            0            0     10    1     1       0   \n",
       "3           0           0            1            0     10    1     1       0   \n",
       "4           0           0            0            1     10    1     1       0   \n",
       "\n",
       "   second  weekday  \n",
       "0       1        1  \n",
       "1       6        1  \n",
       "2       8        1  \n",
       "3       8        1  \n",
       "4       9        1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datecolumn = df.TX_DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatetimeArray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-189ad138fc93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatecolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatecolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The provided date column cannot be parsed. Please provide a datetime column in pandas DatetimeArray format.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatetimeArray' is not defined"
     ]
    }
   ],
   "source": [
    "if not isinstance(datecolumn, DatetimeArray):\n",
    "    try:\n",
    "        DatetimeArray(datecolumn)\n",
    "    except:\n",
    "        print(\"The provided date column cannot be parsed. Please provide a datetime column in pandas DatetimeArray format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inductiveGRL.timeframes import Timeframes\n",
    "\n",
    "from datetime import timedelta\n",
    "tt = Timeframes(DatetimeArray(df.TX_DATETIME), step_size=timedelta(days=1), train_size=timedelta(days=5), test_size=timedelta(days=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([     0,      1,      2, ..., 503287, 503288, 503289]),\n",
       "  array([100669, 100670, 100671, ..., 603732, 603733, 603734]),\n",
       "  array([201593, 201594, 201595, ..., 704306, 704307, 704308]),\n",
       "  array([302419, 302420, 302421, ..., 805115, 805116, 805117]),\n",
       "  array([402834, 402835, 402836, ..., 905452, 905453, 905454]),\n",
       "  array([ 503290,  503291,  503292, ..., 1006112, 1006113, 1006114]),\n",
       "  array([ 603735,  603736,  603737, ..., 1106601, 1106602, 1106603]),\n",
       "  array([ 704309,  704310,  704311, ..., 1206908, 1206909, 1206910]),\n",
       "  array([ 805118,  805119,  805120, ..., 1307189, 1307190, 1307191]),\n",
       "  array([ 905455,  905456,  905457, ..., 1407616, 1407617, 1407618]),\n",
       "  array([1006115, 1006116, 1006117, ..., 1508100, 1508101, 1508102]),\n",
       "  array([1106604, 1106605, 1106606, ..., 1608511, 1608512, 1608513]),\n",
       "  array([1206911, 1206912, 1206913, ..., 1708823, 1708824, 1708825]),\n",
       "  array([1307192, 1307193, 1307194, ..., 1809115, 1809116, 1809117]),\n",
       "  array([1407619, 1407620, 1407621, ..., 1909349, 1909350, 1909351]),\n",
       "  array([1508103, 1508104, 1508105, ..., 2009559, 2009560, 2009561]),\n",
       "  array([1608514, 1608515, 1608516, ..., 2109854, 2109855, 2109856]),\n",
       "  array([1708826, 1708827, 1708828, ..., 2210443, 2210444, 2210445]),\n",
       "  array([1809118, 1809119, 1809120, ..., 2310869, 2310870, 2310871]),\n",
       "  array([1909352, 1909353, 1909354, ..., 2411431, 2411432, 2411433]),\n",
       "  array([2009562, 2009563, 2009564, ..., 2512373, 2512374, 2512375]),\n",
       "  array([2109857, 2109858, 2109859, ..., 2613460, 2613461, 2613462]),\n",
       "  array([2210446, 2210447, 2210448, ..., 2717896, 2717897, 2717898]),\n",
       "  array([2310872, 2310873, 2310874, ..., 2820523, 2820524, 2820525]),\n",
       "  array([2411434, 2411435, 2411436, ..., 2922275, 2922276, 2922277]),\n",
       "  array([2512376, 2512377, 2512378, ..., 3023247, 3023248, 3023249]),\n",
       "  array([2613463, 2613464, 2613465, ..., 3124031, 3124032, 3124033]),\n",
       "  array([2717899, 2717900, 2717901, ..., 3223703, 3223704, 3223705]),\n",
       "  array([2820526, 2820527, 2820528, ..., 3323890, 3323891, 3323892]),\n",
       "  array([2922278, 2922279, 2922280, ..., 3424766, 3424767, 3424768]),\n",
       "  array([3023250, 3023251, 3023252, ..., 3525567, 3525568, 3525569])],\n",
       " [array([503290, 503291, 503292, ..., 603732, 603733, 603734]),\n",
       "  array([603735, 603736, 603737, ..., 704306, 704307, 704308]),\n",
       "  array([704309, 704310, 704311, ..., 805115, 805116, 805117]),\n",
       "  array([805118, 805119, 805120, ..., 905452, 905453, 905454]),\n",
       "  array([ 905455,  905456,  905457, ..., 1006112, 1006113, 1006114]),\n",
       "  array([1006115, 1006116, 1006117, ..., 1106601, 1106602, 1106603]),\n",
       "  array([1106604, 1106605, 1106606, ..., 1206908, 1206909, 1206910]),\n",
       "  array([1206911, 1206912, 1206913, ..., 1307189, 1307190, 1307191]),\n",
       "  array([1307192, 1307193, 1307194, ..., 1407616, 1407617, 1407618]),\n",
       "  array([1407619, 1407620, 1407621, ..., 1508100, 1508101, 1508102]),\n",
       "  array([1508103, 1508104, 1508105, ..., 1608511, 1608512, 1608513]),\n",
       "  array([1608514, 1608515, 1608516, ..., 1708823, 1708824, 1708825]),\n",
       "  array([1708826, 1708827, 1708828, ..., 1809115, 1809116, 1809117]),\n",
       "  array([1809118, 1809119, 1809120, ..., 1909349, 1909350, 1909351]),\n",
       "  array([1909352, 1909353, 1909354, ..., 2009559, 2009560, 2009561]),\n",
       "  array([2009562, 2009563, 2009564, ..., 2109854, 2109855, 2109856]),\n",
       "  array([2109857, 2109858, 2109859, ..., 2210443, 2210444, 2210445]),\n",
       "  array([2210446, 2210447, 2210448, ..., 2310869, 2310870, 2310871]),\n",
       "  array([2310872, 2310873, 2310874, ..., 2411431, 2411432, 2411433]),\n",
       "  array([2411434, 2411435, 2411436, ..., 2512373, 2512374, 2512375]),\n",
       "  array([2512376, 2512377, 2512378, ..., 2613460, 2613461, 2613462]),\n",
       "  array([2613463, 2613464, 2613465, ..., 2717896, 2717897, 2717898]),\n",
       "  array([2717899, 2717900, 2717901, ..., 2820523, 2820524, 2820525]),\n",
       "  array([2820526, 2820527, 2820528, ..., 2922275, 2922276, 2922277]),\n",
       "  array([2922278, 2922279, 2922280, ..., 3023247, 3023248, 3023249]),\n",
       "  array([3023250, 3023251, 3023252, ..., 3124031, 3124032, 3124033]),\n",
       "  array([3124034, 3124035, 3124036, ..., 3223703, 3223704, 3223705]),\n",
       "  array([3223706, 3223707, 3223708, ..., 3323890, 3323891, 3323892]),\n",
       "  array([3323893, 3323894, 3323895, ..., 3424766, 3424767, 3424768]),\n",
       "  array([3424769, 3424770, 3424771, ..., 3525567, 3525568, 3525569]),\n",
       "  array([3525570, 3525571, 3525572, ..., 3626724, 3626725, 3626726])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tt.fold_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.arrays import DatetimeArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatetimeArray(df.TX_DATETIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = pd.to_datetime(df.TX_DATETIME).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(da < dd[5]).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[5] < da[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.TX_DATETIME[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Selecting a Timeframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the robustness of our analysis, we devised a manual 5-fold out-of-time validation, by dividing the dataset based\n",
    "on a rolling window approach. Our setup has 5 timeframes that roll with an interval of 5 days and a total window size of 17 days. The last 5 days of the timeframe are used as the inductive set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inductiveGRL.timeframes import Timeframes\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "tf = Timeframes(df['TX_DATETIME'],step_size=5, window_size=17)\n",
    "timeframe_indices = tf.get_timeframe_indices(timeframe)\n",
    "\n",
    "print('number of days in dataset: ',tf.get_number_of_days())\n",
    "print('number of timeframes derived from window and step size: ',tf.get_number_of_timeframes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `train_data` variable stores the data that will be used to construct graphs on which the representation learners can train. \n",
    "the `inductive_data` will be used to test the inductive performance of our representation learning algorithms. `days_to_hold_out` specifies the number of days we want to hold out at the end of the timeframe for our inductive set. When setting this equal to the step_size in the previous cell, there is no overlap between the different inductive sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_hold_out = 5\n",
    "train_data, inductive_data = tf.train_inductive_split(df.loc[timeframe_indices],days_to_hold_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Selecting an Undersampling Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The distribution of fraud for the train data is:\\n', train_data['TX_FRAUD'].value_counts())\n",
    "print('The distribution of fraud for the inductive data is:\\n', inductive_data['TX_FRAUD'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the highly imbalanced nature of our dataset, we undersample the train data with the aforespecified rate (if any). We also make sure that the indices do not change while undersampling, since these will be used as transaction node identifiers in the graph construction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "if not undersampling_rate is None:\n",
    "    print(\"An undersampling rate of \", undersampling_rate, \"is applied.\")\n",
    "    train_data['index'] = train_data.index\n",
    "    undersample = RandomUnderSampler(sampling_strategy=(undersampling_rate))\n",
    "    X, y = undersample.fit_resample(train_data, train_data['TX_FRAUD'])\n",
    "    train_data = X.set_index(X['index']).drop('index',axis=1)\n",
    "    print('The new distribution for the train set is:\\n', train_data[\"TX_FRAUD\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct the Credit Card Transaction Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nodes, edges and features are passed to the GraphConstruction constructor. Note that client and merchant node data hold a trivial attribute with value 1. This because we want all the relevant transaction data to reside at the transaction nodes and StellarGraph's current HinSAGE implementation requires all nodes to have features. Note that a graph is constructed for a specific timeframe and sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from inductiveGRL.graphconstruction import GraphConstruction\n",
    "\n",
    "transaction_node_data = train_data.drop(\"CARD_PAN_ID\", axis=1).drop(\"TERM_MIDUID\", axis=1).drop(\"TX_FRAUD\", axis=1).drop(\"TX_DATETIME\", axis=1)\n",
    "client_node_data = pd.DataFrame([1]*len(train_data.CARD_PAN_ID.unique())).set_index(train_data.CARD_PAN_ID.unique())\n",
    "merchant_node_data = pd.DataFrame([1]*len(train_data.TERM_MIDUID.unique())).set_index(train_data.TERM_MIDUID.unique())\n",
    "\n",
    "nodes = {\"client\":train_data.CARD_PAN_ID, \"merchant\":train_data.TERM_MIDUID, \"transaction\":train_data.index}\n",
    "edges = [zip(train_data.CARD_PAN_ID, train_data.index),zip(train_data.TERM_MIDUID, train_data.index)]\n",
    "features = {\"transaction\": transaction_node_data, 'client': client_node_data, 'merchant': merchant_node_data}\n",
    "\n",
    "graph = GraphConstruction(nodes, edges, features)\n",
    "S = graph.get_stellargraph()\n",
    "print(S.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Train GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HinSAGE, a heterogeneous implementation of the GraphSAGE framework is trained with user specified hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inductiveGRL.hinsage import HinSAGE_Representation_Learner\n",
    "\n",
    "#GraphSAGE parameters\n",
    "num_samples = [2,32]\n",
    "embedding_node_type = \"transaction\"\n",
    "\n",
    "hinsage = HinSAGE_Representation_Learner(embedding_size, num_samples, embedding_node_type)\n",
    "trained_hinsage_model, graphsage_train_emb = hinsage.train_hinsage(S, list(train_data.index), train_data['TX_FRAUD'], batch_size=50, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Train FIGRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FI-GRL, a fast inductive graph representation framework is trained using the aforeconstructed graph. This algorithm is implemented in matlab so we make use of matlab.engine to deploy its native implementation. First, we instantiate the FI-GRL class with the intermediate dimension of the matrix between the input graph and the embedding space, in addition to the size of final dimension (embedding space). FI-GRL's train step returns three matrices: U, which represents the embedding space, sigma and v, which are matrices that will be used in the inductive step to generate embeddings for unseen nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "#FIGRL hyperparameter\n",
    "intermediate_dim = 400\n",
    "\n",
    "#Instantiate FI-GRL\n",
    "figrl = eng.FIGRL(float(intermediate_dim), embedding_size)\n",
    "\n",
    "#Run train step\n",
    "edges = matlab.double(graph.get_edgelist())\n",
    "U, sigma,v = eng.train_step_figrl(figrl, edges, nargout = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the embeddings returned by Matlab to a pandas dataframe and select the embeddings of our train nodes. Since FI-GRL assumes homogeneous input graphs, it also generated embeddings for nodes that we are not interested in (clients and merchants). We also correct for an index shift of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figrl_train_emb = pd.DataFrame(U)\n",
    "figrl_train_emb = figrl_train_emb.set_index(figrl_train_emb.index+1)\n",
    "figrl_train_emb = figrl_train_emb.loc[train_data.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Inductive Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep the original indices after concatenating the train and inductive data, because they represent the transaction node ids. We need to concatenate these dataframes in order to easily construct the new graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['index'] = train_data.index\n",
    "inductive_data['index'] = inductive_data.index\n",
    "inductive_graph_data = pd.concat((train_data,inductive_data))\n",
    "inductive_graph_data = inductive_graph_data.set_index(inductive_graph_data['index']).drop(\"index\",axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inductive step, we need to add the new, unseen transactions to the graph. Because the current StellarGraph implementation does not support adding nodes and edges to an existing stellargraph object, we create a new graph that contains all the nodes from the train graph in addition to the new nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inductiveGRL.graphconstruction import GraphConstruction\n",
    "\n",
    "transaction_node_data = inductive_graph_data.drop(\"CARD_PAN_ID\", axis=1).drop(\"TERM_MIDUID\", axis=1).drop(\"TX_FRAUD\", axis=1).drop(\"TX_DATETIME\", axis=1)\n",
    "client_node_data = pd.DataFrame([1]*len(inductive_graph_data.CARD_PAN_ID.unique())).set_index(inductive_graph_data.CARD_PAN_ID.unique())\n",
    "merchant_node_data = pd.DataFrame([1]*len(inductive_graph_data.TERM_MIDUID.unique())).set_index(inductive_graph_data.TERM_MIDUID.unique())\n",
    "\n",
    "nodes = {\"client\":inductive_graph_data.CARD_PAN_ID, \"merchant\":inductive_graph_data.TERM_MIDUID, \"transaction\":inductive_graph_data.index}\n",
    "edges = [zip(inductive_graph_data.CARD_PAN_ID, inductive_graph_data.index),zip(inductive_graph_data.TERM_MIDUID, inductive_graph_data.index)]\n",
    "features = {\"transaction\": transaction_node_data, 'client': client_node_data, 'merchant': merchant_node_data}\n",
    "\n",
    "graph = GraphConstruction(nodes, edges, features)\n",
    "S = graph.get_stellargraph()\n",
    "print(S.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Inductive Step GraphSAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inductive step applies the previously learned (and optimized) aggregation functions, part of the `trained_hinsage_model`. We also pass the new graph S and the node identifiers (inductive_data.index) to the inductive step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_inductive_emb = hinsage.inductive_step_hinsage(S, trained_hinsage_model, inductive_data.index, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Inductive Step FI-GRL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inductive step performs computations with the new adjacency matrix and the during training calculated matrices sigma and v. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = matlab.double(graph.get_edgelist())\n",
    "figrl_inductive_emb = eng.inductive_step_figrl(figrl, edges, sigma, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the train step, we extract the embeddings from the nodes we are interested in (i.e. the transaction nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figrl_inductive_emb = pd.DataFrame(figrl_inductive_emb)\n",
    "figrl_inductive_emb = figrl_inductive_emb.set_index(figrl_inductive_emb.index+1)\n",
    "figrl_inductive_emb = figrl_inductive_emb.loc[inductive_data.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification: predictions based on inductive embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your preferred classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If requested, the original transaction features are added to the generated embeddings. If these features are added, a baseline consisting of only these features (without embeddings) is included to analyze the net impact of embeddings on the predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data['TX_FRAUD']\n",
    "\n",
    "if add_additional_data is True:\n",
    "    graphsage_train_emb = pd.merge(graphsage_train_emb, train_data.loc[graphsage_train_emb.index].drop('TX_FRAUD', axis=1).drop('TX_DATETIME', axis=1), left_index=True, right_index=True)\n",
    "    graphsage_inductive_emb = pd.merge(graphsage_inductive_emb, inductive_data.loc[graphsage_inductive_emb.index].drop('TX_FRAUD', axis=1).drop('TX_DATETIME', axis=1), left_index=True, right_index=True)\n",
    "    \n",
    "    figrl_train_emb = pd.merge(figrl_train_emb, train_data.loc[figrl_train_emb.index].drop('TX_FRAUD', axis=1).drop('TX_DATETIME', axis=1), left_index=True, right_index=True)\n",
    "    figrl_inductive_emb = pd.merge(figrl_inductive_emb, inductive_data.loc[figrl_inductive_emb.index].drop('TX_FRAUD', axis=1).drop('TX_DATETIME', axis=1), left_index=True, right_index=True)\n",
    "    \n",
    "    baseline_train = train_data.drop('TX_FRAUD', axis=1).drop('TX_DATETIME', axis=1)\n",
    "    baseline_inductive = inductive_data.drop('TX_FRAUD', axis=1).drop('TX_DATETIME', axis=1)\n",
    "    \n",
    "    classifier.fit(baseline_train, train_labels)\n",
    "    baseline_predictions = classifier.predict_proba(baseline_inductive)\n",
    "\n",
    "classifier.fit(graphsage_train_emb, train_labels)\n",
    "graphsage_predictions = classifier.predict_proba(graphsage_inductive_emb)\n",
    "\n",
    "classifier.fit(figrl_train_emb, train_labels)\n",
    "figrl_predictions = classifier.predict_proba(figrl_inductive_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the highly imbalanced nature of our dataset, we evaluate the results based on precision-recall curves and 1% Lift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inductiveGRL.evaluation import Evaluation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "inductive_labels = df.loc[inductive_data.index]['TX_FRAUD']\n",
    "\n",
    "figrl_evaluation = Evaluation(figrl_predictions, inductive_labels, \"FIGRL+features\") \n",
    "graphsage_evaluation = Evaluation(graphsage_predictions, inductive_labels, \"GraphSAGE+features\")\n",
    "\n",
    "figrl_evaluation.pr_curve()\n",
    "graphsage_evaluation.pr_curve()\n",
    "\n",
    "if add_additional_data is True:\n",
    "    baseline_evaluation = Evaluation(baseline_predictions, inductive_labels, \"Baseline\")\n",
    "    baseline_evaluation.pr_curve()\n",
    "\n",
    "print(\"FI-GRL: \")\n",
    "lift_score = figrl_evaluation.lift_score(0.01)\n",
    "print(\"GraphSAGE: \")\n",
    "lift_score = graphsage_evaluation.lift_score(0.01)\n",
    "\n",
    "if add_additional_data is True:\n",
    "    print(\"Baseline: \")\n",
    "    lift_score = baseline_evaluation.lift_score(0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_develop",
   "language": "python",
   "name": "test_develop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.23.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
